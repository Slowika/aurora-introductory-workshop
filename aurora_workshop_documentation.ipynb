{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Aurora inference & fine-tuning in Azure ML\n",
        "\n",
        "This notebook explains the steps implemented in `aurora_demo_core.py`."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Input data\n",
        "\n",
        "#### General requirements\n",
        "\n",
        "Aurora expects a 3D (space + vertical) grid with a consistent spatial resolution. Input data can be interpreted as a *snapshot of the state of the atmosphere* at a particular time.\n",
        "\n",
        "Input variables are unnormalised: the model normalises each variable independently. \n",
        "\n",
        "Input data must be in the [aurora.Batch format](https://microsoft.github.io/aurora/batch.html) that includes:\n",
        "\n",
        "1. **surface-level variables** (a dictionary)\n",
        "2. **static variables** (a dictionary)\n",
        "3. **atmospheric variables** (all at the same collection of pressure levels; a dictionary)\n",
        "4. **metadata** describing these variables: latitudes, longitudes, the pressure levels of the atmospheric variables, and the time when these variables were recorded. This is an [aurora.Metadata](https://microsoft.github.io/aurora/batch.html#batch-metadata) object.\n",
        "\n",
        "The dictionaries of variables map the predefined names of the variables to their numerical values. [See the full list of supported variables and their short names](https://microsoft.github.io/aurora/batch.html#batch-surf-vars).\n",
        "\n",
        "1. The **surface-level variables** must be of the form (<span style='color: purple;'>b</span>, <span style='color: orange;'>t</span>, <span style='color: green;'>h</span>, <span style='color: red;'>w</span>) where <span style='color: purple;'>b</span> is the batch size, <span style='color: orange;'>t</span> is the history dimension, <span style='color: green;'>h</span> is the number of latitudes, and <span style='color: red;'>w</span> is the number of longitudes.\n",
        "\n",
        "All Aurora models produce the prediction for the next time step from the current time step (surface variables that represent it are in `surf_vars[:, 1, :, :]`) and the previous time step (`surf_vars[:, 0, :, :]`). Note that `Metadata.time` corresponds to the current time step. \n",
        "\n",
        "The batch size <span style='color: purple;'>b</span> corresponds to the number of *spatiotemporal samples*. For example, <span style='color: purple;'>b > 1</span> when samples come from different *ensemble members* (estimations based on varying initial conditions) that are used to predict the weather for the same target time. Aurora can consume ensemble members as independent samples stacked along the <span style='color: purple;'>b</span> dimension. In this case, the output contains independent forecasts for all members. Samples from different spatial tiles can also be used in one batch.\n",
        "\n",
        "2. The **static variables** must be of the form (<span style='color: green;'>h</span>, <span style='color: red;'>w</span>) where <span style='color: green;'>h</span> is the number of latitudes and <span style='color: red;'>w</span> the number of longitudes. These variables do not change with time.\n",
        "\n",
        "3. The **atmospheric variables** must be of the form (<span style='color: purple;'>b</span>, <span style='color: orange;'>t</span>, <span style='color: grey;'>c</span>, <span style='color: green;'>h</span>, <span style='color: red;'>w</span>) where <span style='color: purple;'>b</span> is the batch size, <span style='color: orange;'>t</span> the history dimension, <span style='color: grey;'>c</span> the number of pressure levels, <span style='color: green;'>h</span> the number of latitudes, and <span style='color: red;'>w</span> the number of longitudes. All atmospheric variables must contain the same collection of pressure levels in the same order.\n",
        "\n",
        "\n",
        "4. **metadata** contains the following fields:\n",
        "\n",
        "    * `Metadata.lat` is the vector of latitudes. The latitudes must be decreasing. The latitudes can either include both endpoints: `linspace(90, -90, 721)`, or not include the south pole: `linspace(90, -90, 721)[:-1]`. For curvilinear grids, this can also be a matrix, in which case the foregoing conditions apply to every column.\n",
        "\n",
        "    * `Metadata.lon` is the vector of longitudes. The longitudes must be increasing. The longitudes must be in the range `[0, 360)`, so they can include zero and cannot include 360. For curvilinear grids, this can also be a matrix, in which case the foregoing conditions apply to every row.\n",
        "\n",
        "    * `Metadata.atmos_levels` is a tuple of the pressure levels of the atmospheric variables in hPa. Note that these levels must correspond to the order of the atmospheric variables. Note also that `Metadata.atmos_levels` should be a tuple, not a list.\n",
        "\n",
        "    * `Metadata.time` is a tuple with a `datetime.datetime` representing the time of the data (for each sample in the batch). If the batch size <span style='color: purple;'>b</span> is one, then this will be a one-element tuple, e.g. `(datetime(2024, 1, 1, 12, 0),)`. Since all Aurora models require variables for the current and the previous step, `Metadata.time` corresponds to the time of the current step. Specifically, `Metadata.time[i]` corresponds to the time of `Batch.surf_vars[i, -1]`.\n",
        "\n",
        "#### Dummy data (in GPU memory)\n",
        "\n",
        "In the first example, you will create a dummy dataset stored in the GPU memory. You will use a call to `make_lowres_batch` to create your first `aurora.Batch`.\n",
        "\n",
        "The values of all variables are random but the latitude-longitude grid is realistic for a global forecast (17 latitudes × 32 longitudes).   \n",
        "\n",
        "\n",
        "\n",
        "#### ERA5 (in Azure Blob Storage)\n",
        "\n",
        "TODO\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference\n",
        "\n",
        "You will use `run_inference` to load a pretrained Aurora, create input data, run one forward pass or multiple forward passes (if the `rollout_steps` argument is provided), and return the output of Aurora. The output of Aurora will have the same format as the input batch with one exception -- the history dimension <span style='color: orange;'>t</span> will be equal to 1.\n",
        "\n",
        "* `model = AuroraPretrained()`\n",
        "\n",
        "Instantiate the model. `model` has the same architecture as Aurora but the weights are not loaded yet.\n",
        "* `model.load_checkpoint()`\n",
        "\n",
        "Load the pretrained model weights from HuggingFace. We will use a pretrained 0.25° Aurora (a generic ERA5-ish model). Note that `Aurora()` refers to the 0.25° model fine-tuned on IFS HRES instead of the generic version. [See all available model variants](https://huggingface.co/microsoft/aurora/tree/main).\n",
        "\n",
        "The authors recommend `AuroraPretrained()` for predictions based on ERA5 at 0.25°. This variant of the model is also recommended for fine-tuning to new applications.\n",
        "\n",
        "* `model = model.to(device)`\n",
        "\n",
        "Move the model to a GPU (if available).\n",
        "\n",
        "* `model.eval()`\n",
        "\n",
        "Set the model to evaluation mode. This disables dropout and fixes batch normalisation.\n",
        "\n",
        "* `batch = make_lowres_batch(device=device)`\n",
        "\n",
        "Create an input batch using the function you have just defined. \n",
        "\n",
        "Aurora can be used to generate predictions one lead time forward (one step ahead) or multiple lead times forward (by applying the model autoregressively). These two options differ in implementation: they are provided here as two modes in `run_inference`.\n",
        "\n",
        "#### One step prediction\n",
        "\n",
        "* `with torch.inference_mode():`\n",
        "* `prediction = model(batch)`\n",
        "\n",
        "Disable gradient tracking and run one forward pass.\n",
        "\n",
        "#### Autoregressive rollout\n",
        "\n",
        "* `with torch.inference_mode():`\n",
        "*    `preds = [pred.to(\"cpu\") for pred in rollout(model, batch, steps=rollout_steps)]`\n",
        "\n",
        "Run the model for `rollout_steps` steps ahead using its own outputs as inputs at the next step. Move the predictions to a CPU to prevent GPU memory buildup. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning\n",
        "\n",
        "Steps for fine-tuning Aurora (sources: [Aurora GitHub Issues](https://github.com/microsoft/aurora/issues) and [Official docs: Aurora fine-tuning](https://microsoft.github.io/aurora/finetuning.html)):\n",
        "\n",
        "1. Collect data in the form of surface, static and atmospheric variables. For new variables, set normalisation statistics (means and standard deviations). \n",
        "\n",
        "2. Define the loss. The authors of Aurora use a loss function based on Mean Absolute Error. \n",
        "\n",
        "3. Load the pretrained model and set up the optimiser. \n",
        "\n",
        "4. Fine-tuning loop (repeat for several epochs):  \n",
        "\n",
        "    a) Sample a random data point. \n",
        "\n",
        "    b) Run the model to get the model prediction. \n",
        "\n",
        "    c) Normalise predictions and target data.  \n",
        "\n",
        "    d) Compute the loss and update model parameters. \n",
        "    \n",
        "    e) Monitor how the loss changes during fine-tuning. \n",
        "\n",
        "5. Save the fine-tuned model. \n",
        "\n",
        "You will use `run_finetuning` to update all model weights based on the new dataset. \n",
        "\n",
        "First, you will load the pretrained model weights and move them to a GPU (if available). These steps are the same as in the inference function (`run_inference`).\n",
        "\n",
        "```python\n",
        "model = AuroraPretrained()\n",
        "model.load_checkpoint()\n",
        "model = model.to(device)\n",
        "```\n",
        "\n",
        "Set the model to training mode:\n",
        "\n",
        "`model.train()`\n",
        "\n",
        "Create an optimiser that will update all model parameters. In your 'real' experiments, you might want to treat learning rate as a hyperparameter and try out a range of values.\n",
        "\n",
        "`optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)`\n",
        "\n",
        "We will return the model outputs from the last epoch (`last_prediction`) and the values of the loss function from all epochs (`loss_history`). We will later plot the loss values.\n",
        "\n",
        "```python\n",
        "last_prediction = None\n",
        "loss_history: list[float] = []\n",
        "```\n",
        "\n",
        "In the loop below, you will:\n",
        "\n",
        "1. Generate a random input batch (`batch`).\n",
        "2. Clear any gradients from the previous iteration.\n",
        "3. Produce predictions (`prediction`) based on input data (`batch`).\n",
        "4. Compute the loss.\n",
        "5. Compute gradients with respect to all parameters.\n",
        "6. Use the gradients to update all model weights.\n",
        "7. Append the loss value from the current iteration to `loss_history`.\n",
        "8. Print the epoch number and the loss value.\n",
        "\n",
        "```python\n",
        "for step in range(steps):\n",
        "    batch = make_lowres_batch(device=device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    prediction = model(batch)\n",
        "    loss_value = loss(prediction)\n",
        "    loss_value.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    last_loss_value = float(loss_value.detach().cpu().item())\n",
        "    loss_history.append(last_loss_value)\n",
        "\n",
        "    print(f\"[step {step}] loss = {last_loss_value:.4f}\")\n",
        "```\n",
        "\n",
        "After the predefined number of iterations (`steps`), you will return the outputs from the last epoch and the loss values from all epochs.\n",
        "\n",
        "`return last_prediction, loss_history`\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.18",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}