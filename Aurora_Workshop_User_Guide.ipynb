{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Aurora Workshop User Guide\n",
        "\n",
        "This notebook is a **documentation/wiki-style guide** for the workshop codebase:\n",
        "- `0_aurora_workshop.ipynb` (job submission “remote control”)\n",
        "- `run_aurora_job.py` (Azure ML entrypoint)\n",
        "- `aurora_demo_core.py` (model/data “engine”)\n",
        "\n",
        "It explains **what each piece does**, **how the flows work**, **what inputs/outputs are expected**, and **how to troubleshoot common errors**.\n"
      ],
      "metadata": {},
      "id": "935a5670"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Contents\n",
        "1. Overview\n",
        "2. Code layout and responsibilities\n",
        "3. Concepts you need (Batch, variables, lead time, cropping)\n",
        "4. Inputs and data contracts (ERA5 Zarr + static NetCDF)\n",
        "5. Outputs (what files get written and how to interpret them)\n",
        "6. How execution works end-to-end\n",
        "7. Configuration reference (profiles & environment variables)\n",
        "8. Flows explained\n",
        "   - Toy inference / toy fine-tune\n",
        "   - ERA5 short-lead inference\n",
        "   - ERA5 short-lead fine-tuning\n",
        "   - ERA5 rollout (autoregressive) fine-tuning\n",
        "   - LoRA modes (short vs rollout; LoRA-only training)\n",
        "   - “Add variable” demo (EXTRA_* env vars)\n",
        "9. Recipes (copy/paste profiles)\n",
        "10. Troubleshooting (the errors you actually hit)\n",
        "11. Extending the workshop (new vars, new losses, bigger runs)\n"
      ],
      "metadata": {},
      "id": "e37ae754"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1) Overview\n",
        "\n",
        "This workshop code is split into **three layers**:\n",
        "\n",
        "### A) Notebook (controller)\n",
        "You choose a **profile** (toy / era5 short / era5 rollout / LoRA variants), set up Azure ML inputs (datasets),\n",
        "and submit a job. The notebook does **no model logic**.\n",
        "\n",
        "### B) Job runner: `run_aurora_job.py`\n",
        "Azure ML executes this on the compute node. It:\n",
        "- reads environment variables (FLOW, FT_MODE, USE_LORA, etc.)\n",
        "- resolves mounted input paths for ERA5 data\n",
        "- calls the appropriate function in `aurora_demo_core.py`\n",
        "- saves outputs (`*.npz`, `*.npy`, `*.json`, `*.pt`) under `OUT_DIR/<participant_id>/`\n",
        "\n",
        "### C) Core engine: `aurora_demo_core.py`\n",
        "This is the “engine”. It:\n",
        "- builds Aurora batches (toy and ERA5)\n",
        "- constructs `AuroraPretrained` (optionally with LoRA and/or an extra variable)\n",
        "- runs inference\n",
        "- runs fine-tuning loops:\n",
        "  - **short-lead** supervised (single step)\n",
        "  - **rollout** supervised (autoregressive / unrolled)\n"
      ],
      "metadata": {},
      "id": "49407c5b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 2) Code layout and responsibilities\n",
        "\n",
        "### Files you will touch most\n",
        "- **`0_aurora_workshop.ipynb`**  \n",
        "  - defines “profiles” (sets env vars)\n",
        "  - mounts Azure ML inputs:\n",
        "    - dynamic ERA5 Zarr folder (URI_FOLDER)\n",
        "    - static NetCDF (URI_FILE)\n",
        "  - submits the job\n",
        "\n",
        "- **`run_aurora_job.py`**\n",
        "  - reads env vars\n",
        "  - validates required inputs (e.g., when FLOW=era5)\n",
        "  - runs inference\n",
        "  - optionally runs fine-tuning (`FINETUNE_STEPS > 0`)\n",
        "  - writes outputs\n",
        "\n",
        "- **`aurora_demo_core.py`**\n",
        "  - implements toy and ERA5 logic\n",
        "  - key functions:\n",
        "    - `run_inference_era5(...)`\n",
        "    - `run_finetuning_era5_short_lead(...)`\n",
        "    - `run_finetuning_era5_rollout(...)`\n",
        "    - `build_model(...)`\n",
        "    - `make_era5_short_lead_pair(...)`\n"
      ],
      "metadata": {},
      "id": "f712f992"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 3) Key concepts\n",
        "\n",
        "### 3.1 Aurora `Batch` and shapes\n",
        "Aurora uses a `Batch` object with:\n",
        "- `surf_vars`: dict of surface fields\n",
        "- `static_vars`: dict of static fields (2D)\n",
        "- `atmos_vars`: dict of multi-level fields\n",
        "- `metadata`: lat/lon/time/levels\n",
        "\n",
        "**Shape convention used in this workshop:**\n",
        "- Surface history input: `(B, T, H, W)`  \n",
        "- Atmos history input: `(B, T, C, H, W)` where `C` is number of pressure levels\n",
        "- Static vars: `(H, W)` (no time/history dimension)\n",
        "\n",
        "In the code, history length `T` is **2**: `[t-lead, t]`.\n",
        "\n",
        "### 3.2 “Dynamic” vs “Static” variables\n",
        "- **Dynamic** variables change with time and live in your Zarr:\n",
        "  - surface: 2m temperature, 10m winds, MSLP\n",
        "  - atmos: temperature, u/v wind, humidity, geopotential (with levels)\n",
        "- **Static** variables are 2D fields:\n",
        "  - `lsm` (land-sea mask)\n",
        "  - `slt` (soil type)\n",
        "  - `z` (surface geopotential/orography-like field)\n",
        "\n",
        "Aurora expands each 2D static var into `(B, T, H, W)` internally by repeating it.\n",
        "That’s why static vars must be **exactly 2D**.\n",
        "\n",
        "### 3.3 Lead time and indexing\n",
        "You choose a forecast lead time like `ERA5_LEAD_HOURS=6`.\n",
        "The code infers the dataset’s time step (`dt_hours`) from `ds.time[1] - ds.time[0]`.\n",
        "Then it converts lead hours to an index step:\n",
        "\n",
        "`step = lead_hours / dt_hours`\n",
        "\n",
        "### 3.4 Cropping\n",
        "To keep workshop runs small, the code center-crops the ERA5 subset:\n",
        "- `ERA5_CROP_LAT` controls height `H`\n",
        "- `ERA5_CROP_LON` controls width `W`\n",
        "\n",
        "**Important:** static NetCDF is aligned to the (cropped) lat/lon grid.\n"
      ],
      "metadata": {},
      "id": "545740cf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 4) Inputs and data contracts\n",
        "\n",
        "### 4.1 Dynamic ERA5 Zarr input (required for FLOW=era5)\n",
        "The job runner expects `ERA5_ZARR_PATH` to point to a **local mounted folder** that contains a Zarr store.\n",
        "It opens it with `xr.open_zarr(..., consolidated=True)` and then:\n",
        "- standardises lon to `[0, 360)` and sorts\n",
        "- ensures latitude is decreasing (north→south)\n",
        "- optionally selects a set of “Aurora levels” if the dataset has a `level` coordinate\n",
        "- crops to center patch\n",
        "\n",
        "**Variables expected by default (from `aurora_demo_core.py`):**\n",
        "Surface mapping (Aurora key → ERA5 name):\n",
        "- `2t`  → `2m_temperature`\n",
        "- `10u` → `10m_u_component_of_wind`\n",
        "- `10v` → `10m_v_component_of_wind`\n",
        "- `msl` → `mean_sea_level_pressure`\n",
        "\n",
        "Atmos mapping (Aurora key → ERA5 name):\n",
        "- `t` → `temperature`\n",
        "- `u` → `u_component_of_wind`\n",
        "- `v` → `v_component_of_wind`\n",
        "- `q` → `specific_humidity`\n",
        "- `z` → `geopotential`  (on levels)\n",
        "\n",
        "### 4.2 Static NetCDF input (required for FLOW=era5)\n",
        "The job runner expects `ERA5_STATIC_NC` to point to a **local mounted file** (NetCDF) that contains:\n",
        "- `lsm`, `slt`, `z`\n",
        "\n",
        "**Critical shape rule:** Each static variable must be **2D** `(latitude, longitude)`.\n",
        "Your file *may* contain a singleton time dimension, but it must be removable.  \n",
        "In `aurora_demo_core.py`, `_get_static_arrays()` drops `time` if it exists, but it does NOT drop `valid_time`.\n",
        "So if CDS gave you `valid_time`, you must pre-process the file (or rename the dimension).\n",
        "\n",
        "### 4.3 Mounting in Azure ML (what the notebook does)\n",
        "Your notebook mounts:\n",
        "- `era5_zarr` as `URI_FOLDER` → e.g. `.../wd/INPUT_era5_zarr`\n",
        "- `era5_static` as `URI_FILE` → e.g. `.../wd/INPUT_era5_static`\n",
        "\n",
        "Then it passes those mounted paths to the runner via:\n",
        "- `ERA5_ZARR_PATH=<mounted folder>`\n",
        "- `ERA5_STATIC_NC=<mounted file>`\n",
        "\n",
        "If you run locally, you can set these env vars yourself and call:\n",
        "`python run_aurora_job.py`\n"
      ],
      "metadata": {},
      "id": "b955b824"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 5) Outputs produced by `run_aurora_job.py`\n",
        "\n",
        "Outputs are written under:\n",
        "`OUT_DIR/<PARTICIPANT_ID>/`\n",
        "\n",
        "### Always (inference)\n",
        "- `inference_full_prediction.npz`  \n",
        "  Full model prediction saved as NPZ (surf, static, atmos, plus metadata when available).\n",
        "- `inference_2t.npy`  \n",
        "  Convenience dump of predicted surface 2m temperature (if present).\n",
        "\n",
        "### Optional (inference rollout)\n",
        "If `INFER_ROLLOUT_STEPS` is set:\n",
        "- `inference_rollout_2t.npy`  \n",
        "  Stack of predicted 2m temperature across rollout steps.\n",
        "\n",
        "### If `FINETUNE_STEPS > 0` (fine-tuning)\n",
        "- `finetune_last_prediction.npz`  \n",
        "  Final prediction after the last training step.\n",
        "- `finetune_last_2t.npy`  \n",
        "  Final predicted 2m temperature after fine-tuning.\n",
        "- `finetune_losses.npy` + `finetune_losses.json`  \n",
        "  Loss history (one per training step).\n",
        "- `finetuned_state_dict.pt`  \n",
        "  Saved model weights:\n",
        "  - if `USE_LORA=1` and `TRAIN_LORA_ONLY=1`: saved weights are filtered to only LoRA params (by name containing 'lora')\n",
        "  - otherwise: full state_dict\n"
      ],
      "metadata": {},
      "id": "bc7377cf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 6) End-to-end execution flow\n",
        "\n",
        "### Step 1: You pick a profile in the notebook\n",
        "A “profile” is just a dictionary of environment variables.\n",
        "Example idea:\n",
        "- `FLOW=toy` vs `FLOW=era5`\n",
        "- `FT_MODE=short` vs `FT_MODE=rollout`\n",
        "- `FINETUNE_STEPS=0` to skip training\n",
        "\n",
        "### Step 2: Notebook submits Azure ML command job\n",
        "The job runs:\n",
        "`python run_aurora_job.py`\n",
        "\n",
        "and passes env vars + mounts ERA5 inputs.\n",
        "\n",
        "### Step 3: Runner reads env vars and calls core functions\n",
        "The runner:\n",
        "1) runs inference (toy or ERA5)\n",
        "2) if `FINETUNE_STEPS>0`, runs fine-tuning:\n",
        "   - short: `run_finetuning_era5_short_lead(...)`\n",
        "   - rollout: `run_finetuning_era5_rollout(...)`\n",
        "\n",
        "### Step 4: Runner writes outputs and exits\n",
        "Everything you inspect later (loss curves, NPZs, weights) comes from the output directory.\n"
      ],
      "metadata": {},
      "id": "2243cf66"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 7) Configuration reference (profiles & environment variables)\n",
        "\n",
        "This section is your “reference manual”.\n",
        "\n",
        "### 7.1 Top-level switches\n",
        "- `FLOW`: `toy` or `era5`\n",
        "- `FT_MODE` (ERA5 only): `short` or `rollout`\n",
        "- `FINETUNE_STEPS`: 0 means “inference only”\n",
        "\n",
        "### 7.2 LoRA switches\n",
        "- `USE_LORA`:\n",
        "  - default is **True for rollout**, False for short (because runner sets default based on FT_MODE)\n",
        "  - explicitly set `USE_LORA=0` to disable LoRA in rollout runs\n",
        "- `TRAIN_LORA_ONLY`:\n",
        "  - if `USE_LORA=1` and `TRAIN_LORA_ONLY=1`, the code freezes base weights and trains only LoRA params\n",
        "- `LORA_MODE`: `\"single\" | \"from_second\" | \"all\"`\n",
        "- `LORA_STEPS`: passed into Aurora constructor (model configuration; not the training-loop length)\n",
        "\n",
        "### 7.3 Rollout-specific switches\n",
        "- `ROLLOUT_HORIZON_STEPS`: how many autoregressive steps to unroll\n",
        "- `ROLLOUT_LOSS_ON`: `\"last\"` (cheaper) or `\"sum\"` (heavier)\n",
        "\n",
        "### 7.4 “Add one variable” demo\n",
        "If you want to extend the model + inputs with a new variable:\n",
        "- `EXTRA_KIND`: `surf` | `atmos` | `static`\n",
        "- `EXTRA_KEY`: the Aurora variable key you want to add (e.g., `2d` if you invent one)\n",
        "- `EXTRA_SRC`: the variable name in your dataset (Zarr or NetCDF)\n",
        "- `EXTRA_LOCATION`, `EXTRA_SCALE`: normalisation stats registered into Aurora’s normalisation tables\n",
        "\n",
        "\n",
        "### Environment variables table\n",
        "\n",
        "| Variable | Default | Values | Used for |\n",
        "|---|---|---|---|\n",
        "| `FLOW` | toy | toy | era5 | Which flow to run. |\n",
        "| `DEVICE` | cuda (if available) else cpu | cuda | cpu | Torch device. |\n",
        "| `OUT_DIR` | ./outputs | path | Base output directory. |\n",
        "| `PARTICIPANT_ID` | unknown | string | Subfolder under OUT_DIR. |\n",
        "| `ERA5_ZARR_PATH` | (required for era5) | path | Mounted folder containing dynamic ERA5 subset Zarr. |\n",
        "| `ERA5_STATIC_NC` | (required for era5) | path | Mounted NetCDF file containing lsm/slt/z. |\n",
        "| `FT_MODE` | short | short | rollout | Fine-tuning mode (ERA5). |\n",
        "| `FINETUNE_STEPS` | 0 | int | Number of training steps (optimizer updates). |\n",
        "| `LR` | 3e-05 | float | Learning rate for AdamW. |\n",
        "| `AUTOCAST` | True | 0/1 | Enable autocast (mixed precision) inside Aurora. |\n",
        "| `ERA5_TIME_INDEX` | 10 | int | Time index chosen from subset for repeatability. |\n",
        "| `ERA5_LEAD_HOURS` | 6 | int (hours) | Forecast lead time; must divide dataset timestep. |\n",
        "| `ERA5_CROP_LAT` | 128 | int | Center crop height. |\n",
        "| `ERA5_CROP_LON` | 256 | int | Center crop width. |\n",
        "| `INFER_ROLLOUT_STEPS` | unset | int or unset | If set, run autoregressive rollout during inference. |\n",
        "| `USE_LORA` | default: (FT_MODE==rollout) | 0/1 | Enable LoRA adapters. |\n",
        "| `TRAIN_LORA_ONLY` | True | 0/1 | Train only LoRA params (freeze base weights). |\n",
        "| `LORA_MODE` | all for rollout, single for short | single | from_second | all | Where LoRA adapters apply inside the model. |\n",
        "| `LORA_STEPS` | 40 | int | LoRA configuration passed into Aurora (not training loop length). |\n",
        "| `STABILISE_LEVEL_AGG` | False | 0/1 | Enable level aggregation stabilisation in Aurora. |\n",
        "| `ROLLOUT_HORIZON_STEPS` | 8 | int | Autoregressive unroll length during rollout fine-tuning. |\n",
        "| `ROLLOUT_LOSS_ON` | last | last | sum | Loss strategy during rollout fine-tuning. |\n",
        "| `EXTRA_KIND` | unset | surf | atmos | static | Add one extra variable (optional). |\n",
        "| `EXTRA_KEY` | unset | string | Aurora key name for extra variable. |\n",
        "| `EXTRA_SRC` | unset | string | Dataset variable name for extra variable. |\n",
        "| `EXTRA_LOCATION` | 0.0 | float | Normalisation location for new variable. |\n",
        "| `EXTRA_SCALE` | 1.0 | float | Normalisation scale for new variable. |\n"
      ],
      "metadata": {},
      "id": "f9ada7a8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 8) Flows explained\n",
        "\n",
        "### 8.1 Toy flow (`FLOW=toy`)\n",
        "Toy flow is a sanity check:\n",
        "- creates a small synthetic batch (`make_lowres_batch()`)\n",
        "- runs inference (`run_inference()`)\n",
        "- optionally runs a tiny fine-tune (`run_finetuning()`)\n",
        "\n",
        "Use this when:\n",
        "- you just built a new environment/Docker image\n",
        "- you want to confirm CUDA works\n",
        "- you want to confirm the job infrastructure works (outputs, logging)\n",
        "\n",
        "### 8.2 ERA5 short-lead inference (`FLOW=era5`, `FT_MODE=short`, `FINETUNE_STEPS=0`)\n",
        "- builds a single input batch `x` with history length `T=2`: `[t-lead, t]`\n",
        "- runs `model(x)`\n",
        "- saves prediction to NPZ and dumps `2t` to NPY\n",
        "\n",
        "If you set `INFER_ROLLOUT_STEPS=N`, inference switches to a pure inference rollout:\n",
        "- calls `aurora.rollout(model, x, steps=N)`\n",
        "- saves a rollout stack for plotting\n",
        "\n",
        "### 8.3 ERA5 short-lead fine-tuning (`FT_MODE=short`, `FINETUNE_STEPS>0`)\n",
        "- builds `(x, y)` pair:\n",
        "  - `x` contains `[t-lead, t]`\n",
        "  - `y` contains `[t+lead]`\n",
        "- trains for `FINETUNE_STEPS` optimizer updates on one sample\n",
        "- loss used: `supervised_mae(pred, y)`\n",
        "- optionally uses LoRA:\n",
        "  - if `USE_LORA=1` and `TRAIN_LORA_ONLY=1`, freezes base params and trains LoRA params only\n",
        "\n",
        "This is the simplest “hello-world” fine-tune.\n",
        "\n",
        "### 8.4 ERA5 rollout (autoregressive) fine-tuning (`FT_MODE=rollout`)\n",
        "This is the autoregressive training loop.\n",
        "It unrolls multiple steps and feeds predictions back as the next input state:\n",
        "\n",
        "- Build initial batch `x0` with history `[t-lead, t]`.\n",
        "- Build target batches for each rollout step:\n",
        "  - targets at `t+lead`, `t+2*lead`, ..., `t+K*lead`\n",
        "- For each training iteration:\n",
        "  - predict step by step\n",
        "  - update the batch history by shifting the time axis and appending predicted fields\n",
        "  - compute loss either:\n",
        "    - `sum`: loss at every step (bigger graph)\n",
        "    - `last`: no-grad for first K-1 steps; loss only at final step (lighter)\n",
        "\n",
        "**Important:** Rollout fine-tuning can be run with or without LoRA.\n",
        "Set `USE_LORA=0` explicitly if you want full-model rollout training.\n",
        "\n",
        "### 8.5 LoRA: `LORA_MODE` and `LORA_STEPS`\n",
        "- `LORA_MODE` controls where LoRA adapters apply inside Aurora.\n",
        "- `LORA_STEPS` is passed into the Aurora constructor as configuration.\n",
        "In this workshop code, the training-loop length is always `FINETUNE_STEPS`, not `LORA_STEPS`.\n",
        "\n",
        "### 8.6 “Add variable” demo\n",
        "This code supports adding **one new variable** by:\n",
        "1) extending the variable tuples passed to `AuroraPretrained`\n",
        "2) registering normalisation stats in Aurora’s normalisation tables\n",
        "\n",
        "You must:\n",
        "- create a PLUS dataset (Zarr/NetCDF containing your new variable)\n",
        "- mount it\n",
        "- set `EXTRA_*` env vars\n"
      ],
      "metadata": {},
      "id": "39b1a6eb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 9) Recipes: profiles you can copy/paste\n",
        "\n",
        "### 9.1 ERA5 short-lead (no training)\n",
        "```python\n",
        "{\n",
        "  \"FLOW\": \"era5\",\n",
        "  \"FT_MODE\": \"short\",\n",
        "  \"FINETUNE_STEPS\": \"0\",\n",
        "  \"ERA5_LEAD_HOURS\": \"6\",\n",
        "  \"ERA5_TIME_INDEX\": \"10\",\n",
        "  \"ERA5_CROP_LAT\": \"128\",\n",
        "  \"ERA5_CROP_LON\": \"256\",\n",
        "  \"AUTOCAST\": \"1\",\n",
        "}\n",
        "```\n",
        "\n",
        "### 9.2 ERA5 short-lead fine-tune (full model, no LoRA)\n",
        "```python\n",
        "{\n",
        "  \"FLOW\": \"era5\",\n",
        "  \"FT_MODE\": \"short\",\n",
        "  \"FINETUNE_STEPS\": \"50\",\n",
        "  \"USE_LORA\": \"0\",\n",
        "  \"LR\": \"3e-5\",\n",
        "  \"ERA5_LEAD_HOURS\": \"6\",\n",
        "  \"ERA5_TIME_INDEX\": \"10\",\n",
        "  \"ERA5_CROP_LAT\": \"128\",\n",
        "  \"ERA5_CROP_LON\": \"256\",\n",
        "  \"AUTOCAST\": \"1\",\n",
        "}\n",
        "```\n",
        "\n",
        "### 9.3 ERA5 short-lead fine-tune (LoRA-only)\n",
        "```python\n",
        "{\n",
        "  \"FLOW\": \"era5\",\n",
        "  \"FT_MODE\": \"short\",\n",
        "  \"FINETUNE_STEPS\": \"100\",\n",
        "  \"USE_LORA\": \"1\",\n",
        "  \"TRAIN_LORA_ONLY\": \"1\",\n",
        "  \"LORA_MODE\": \"single\",\n",
        "  \"LORA_STEPS\": \"40\",\n",
        "  \"LR\": \"3e-4\",\n",
        "  \"AUTOCAST\": \"1\",\n",
        "}\n",
        "```\n",
        "\n",
        "### 9.4 ERA5 rollout fine-tune WITHOUT LoRA (full model autoregressive)\n",
        "```python\n",
        "{\n",
        "  \"FLOW\": \"era5\",\n",
        "  \"FT_MODE\": \"rollout\",\n",
        "  \"FINETUNE_STEPS\": \"20\",\n",
        "  \"USE_LORA\": \"0\",\n",
        "  \"ROLLOUT_HORIZON_STEPS\": \"8\",\n",
        "  \"ROLLOUT_LOSS_ON\": \"last\",\n",
        "  \"LR\": \"3e-5\",\n",
        "  \"AUTOCAST\": \"1\",\n",
        "}\n",
        "```\n",
        "\n",
        "### 9.5 ERA5 rollout fine-tune WITH LoRA-only (fast adaptation)\n",
        "```python\n",
        "{\n",
        "  \"FLOW\": \"era5\",\n",
        "  \"FT_MODE\": \"rollout\",\n",
        "  \"FINETUNE_STEPS\": \"50\",\n",
        "  \"USE_LORA\": \"1\",\n",
        "  \"TRAIN_LORA_ONLY\": \"1\",\n",
        "  \"LORA_MODE\": \"all\",\n",
        "  \"LORA_STEPS\": \"40\",\n",
        "  \"ROLLOUT_HORIZON_STEPS\": \"8\",\n",
        "  \"ROLLOUT_LOSS_ON\": \"last\",\n",
        "  \"LR\": \"3e-4\",\n",
        "  \"AUTOCAST\": \"1\",\n",
        "}\n",
        "```\n"
      ],
      "metadata": {},
      "id": "b838fe2d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 10) Troubleshooting (common errors and fixes)\n",
        "\n",
        "### A) Static vars shape error (repeat dims / wrong dims)\n",
        "**Symptom:**\n",
        "`RuntimeError: Number of dimensions of repeat dims can not be smaller...`\n",
        "\n",
        "**Cause:**\n",
        "Aurora expects static vars as 2D `(H, W)`, but you provided `(1, H, W)` or `(valid_time, H, W)`.\n",
        "\n",
        "**Fix:**\n",
        "Ensure the static NetCDF has 2D `lsm/slt/z` or at least a removable singleton time dimension.\n",
        "If CDS gives you `valid_time`, drop it when creating the file.\n",
        "\n",
        "### B) Static vars are NaN\n",
        "**Likely causes:**\n",
        "- reading the wrong variable from ARCO store\n",
        "- misaligned lon convention (-180..180 vs 0..360)\n",
        "- selecting the wrong coordinates (empty selection or wrong nearest)\n",
        "\n",
        "**Fix:**\n",
        "Standardise lon to 0..360 on both sides and align to the dynamic grid.\n",
        "\n",
        "### C) LoRA checkpoint loading “missing lora_A/lora_B keys”\n",
        "**Symptom:**\n",
        "`Missing key(s) in state_dict: ... lora_A ... lora_B ...`\n",
        "\n",
        "**Cause:**\n",
        "You built a LoRA-enabled model but tried to load a base checkpoint with `strict=True`.\n",
        "\n",
        "**Fix:**\n",
        "When LoRA is enabled, load base checkpoint with `strict=False` so LoRA weights can stay initialised.\n",
        "\n",
        "### D) Cross-device rename error\n",
        "**Symptom:**\n",
        "`Invalid cross-device link` when moving `/tmp/...` to `/mnt/...`\n",
        "\n",
        "**Fix:**\n",
        "Write temp files into the same directory as the final destination and use `os.replace()`.\n",
        "\n",
        "### E) “FLOW=era5 requires ERA5_ZARR_PATH and ERA5_STATIC_NC”\n",
        "**Fix:**\n",
        "Your job didn’t mount the inputs or didn’t pass the env vars. Check the notebook job definition.\n"
      ],
      "metadata": {},
      "id": "42af4b6c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 11) Extending the workshop\n",
        "\n",
        "### 11.1 Larger crops / larger horizons\n",
        "- Increase `ERA5_CROP_LAT/LON` carefully (memory grows with H×W×levels)\n",
        "- Increase `ROLLOUT_HORIZON_STEPS` for longer autoregressive sequences\n",
        "\n",
        "### 11.2 Multiple samples / real training\n",
        "Right now short-lead and rollout fine-tune loops train on **one chosen time index** (repeatable workshop demo).\n",
        "To make it “real”, you would:\n",
        "- iterate over many `time_index` values\n",
        "- shuffle and batch them\n",
        "- checkpoint periodically\n",
        "\n",
        "### 11.3 Add variables cleanly\n",
        "Use the `EXTRA_*` mechanism and register normalisation stats. Then:\n",
        "- expand your Zarr/NetCDF to contain the new variable\n",
        "- mount PLUS dataset\n",
        "- set env vars in profile\n",
        "\n",
        "### 11.4 Alternative loss functions\n",
        "Replace or extend `supervised_mae` with:\n",
        "- weighted MAE by variable\n",
        "- pressure-level weighting\n",
        "- spatial masks (land-only, etc.)\n"
      ],
      "metadata": {},
      "id": "c9c75484"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Appendix: quick output inspection"
      ],
      "metadata": {},
      "id": "9323e0d3"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pathlib import Path\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# Point this at your downloaded job output folder\n",
        "participant_dir = Path(\"job_outputs/<JOB_NAME>/out_dir/<PARTICIPANT_ID>\")  # <- edit\n",
        "\n",
        "print(\"Participant dir:\", participant_dir.resolve())\n",
        "print(\"Files:\", [p.name for p in participant_dir.iterdir()])\n",
        "\n",
        "# Losses\n",
        "loss_json = participant_dir / \"finetune_losses.json\"\n",
        "if loss_json.exists():\n",
        "    losses = json.loads(loss_json.read_text())\n",
        "    print(\"Loss steps:\", len(losses))\n",
        "    print(\"First/last:\", losses[0], losses[-1])\n",
        "\n",
        "# 2m temperature arrays\n",
        "t2_inf = participant_dir / \"inference_2t.npy\"\n",
        "if t2_inf.exists():\n",
        "    arr = np.load(t2_inf)\n",
        "    print(\"inference_2t.npy shape:\", arr.shape, \"min/max:\", float(arr.min()), float(arr.max()))\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "87bf3b95"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.18",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}