{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e69381ca-fa9b-4cdb-9bbf-971650c14107",
      "metadata": {},
      "source": [
        "# Aurora inference & fine-tuning in Azure ML\n",
        "\n",
        "This notebook explains the workflow and submits an Azure Machine Learning (AML) job that runs **Microsoft Aurora** on a GPU cluster.\n",
        "\n",
        "## Files used in this workshop\n",
        "\n",
        "We use the following components:\n",
        "\n",
        "1. `notebooks/0_aurora_workshop.ipynb` *(this notebook)* – explains the workflow and submits jobs to AML:\n",
        "   - <mark>Run this notebook with a CPU Compute Instance using the \"Python 3.10 - SDK v2\" kernel</mark>\n",
        "2. `setup/components/inference` - contains Aurora inference logic:\n",
        "   - `main.py`: a script with a CLI interface for running a simple inference loop.\n",
        "   - `component.py`: AML component definition.\n",
        "3. `setup/components/training` - contains Aurora fine-tuning logic:\n",
        "   - `main.py`: a script with a CLI interface for running a simple fine-tuning loop.\n",
        "   - `component.py`: AML component definition.\n",
        "4. `setup/components/common/utils.py` - contains Aurora helper logic, including:\n",
        "   - Loading model checkpoints in train or eval mode.\n",
        "   - Loading data on disk into `aurora.Batch` objects for inference and fine-tuning.\n",
        "   - Converting `aurora.Batch` objects into `xarray.Dataset` objects for analysis and writing of data.\n",
        "\n",
        "**NOTE**: inference and fine-tuning scripts in `setup/components/*/main.py` will work in local and remote environments provided the hardware and dependencies required to run Aurora are present in each. AML component definitions in `setup/components/*/components.py` serve only to deploy and make these scripts executable in AML."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "338243af",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import sys\n",
        "from datetime import UTC, datetime\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "import yaml\n",
        "from azure.ai.ml import Input, Output, PyTorchDistribution\n",
        "from azure.ai.ml.entities import Command, CommandComponent, CommandJobLimits\n",
        "from azure.ai.ml.exceptions import JobException, MlException\n",
        "\n",
        "sys.path.insert(0, str(Path.cwd().parent.resolve()))\n",
        "from setup.common.utils import create_mlclient, get_latest_asset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d553f539",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, duncan!\n"
          ]
        }
      ],
      "source": [
        "PARTICIPANT_ID = input(\"Enter a participant ID e.g. saadatali\").strip()\n",
        "print(f\"Hello, {PARTICIPANT_ID}!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c30758e",
      "metadata": {},
      "source": [
        "Create an `azure.ai.ml.MLClient` object to interact with the workspace and, with this, retrieve the compute cluster, model, and data required to run jobs.\n",
        "\n",
        "**NOTE**: the `local` parameter expects a boolean argument that decides what environment variables to look for when configuring the `MLClient` object. `True` will look for environment variables set in a local `.env` file in the project root, `False` will look for environment variables automatically set in Azure Machine Learning Compute Instances. See `setup/common/utils.py` for more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "7a5afeef",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Overriding of current TracerProvider is not allowed\n",
            "Overriding of current LoggerProvider is not allowed\n",
            "Overriding of current MeterProvider is not allowed\n",
            "Attempting to instrument while already instrumented\n",
            "Attempting to instrument while already instrumented\n",
            "Attempting to instrument while already instrumented\n",
            "Attempting to instrument while already instrumented\n",
            "Attempting to instrument while already instrumented\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connected to workspace: sub=62118f5c-be37-400f-9f20-a8b77a2a7877, rg=data-science-team-rg, workspace=data-science-team-workspace\n",
            "Using assets: cluster=test-cluster, model=azureml:aurora-0p25-pretrained:2, data=azureml:workshop-test-asset:7\n"
          ]
        }
      ],
      "source": [
        "ml_client = create_mlclient(local=True)\n",
        "print(\n",
        "    f\"Connected to workspace: sub={ml_client.subscription_id}, \"\n",
        "    f\"rg={ml_client.resource_group_name}, workspace={ml_client.workspace_name}\",\n",
        ")\n",
        "\n",
        "# get the name of the first AML compute cluster (type=\"amlcompute\") in the workspace\n",
        "CLUSTER_NAME = next(iter(ml_client.compute.list(compute_type=\"amlcompute\"))).name\n",
        "\n",
        "# get the latest pre-trained Aurora 0.25 model registered in the workspace\n",
        "model = get_latest_asset(ml_client.models, name=\"aurora-0p25-pretrained\")\n",
        "MODEL_NAME = f\"azureml:{model.name}:{model.version}\"\n",
        "\n",
        "# get the latest ERA5 subset data asset registered in the workspace\n",
        "data = get_latest_asset(ml_client.data, name=\"workshop-test-asset\")\n",
        "DATA_NAME = f\"azureml:{data.name}:{data.version}\"\n",
        "\n",
        "print(f\"Using assets: cluster={CLUSTER_NAME}, model={MODEL_NAME}, data={DATA_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "767c8213",
      "metadata": {},
      "source": [
        "## Test data inference job\n",
        "\n",
        "Here, we'll run a test inference job using generated synthetic test data comprising a low resolution tensor of random float values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "47fdfc99",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
            "Class AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Submitting test inference job: duncan-20260116-114211\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Class BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
            "Class IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
            "Class ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
            "Class BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
            "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFileJobOutput'> and will be ignored\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Streaming logs:\n",
            "RunId: placid_feather_9m5ybcl5w7\n",
            "Web View: https://ml.azure.com/runs/placid_feather_9m5ybcl5w7?wsid=/subscriptions/62118f5c-be37-400f-9f20-a8b77a2a7877/resourcegroups/data-science-team-rg/workspaces/data-science-team-workspace\n",
            "\n",
            "Execution Summary\n",
            "=================\n",
            "RunId: placid_feather_9m5ybcl5w7\n",
            "Web View: https://ml.azure.com/runs/placid_feather_9m5ybcl5w7?wsid=/subscriptions/62118f5c-be37-400f-9f20-a8b77a2a7877/resourcegroups/data-science-team-rg/workspaces/data-science-team-workspace\n",
            "\n"
          ]
        }
      ],
      "source": [
        "display_name = f\"{PARTICIPANT_ID}-{datetime.now(UTC).strftime('%Y%m%d-%H%M%S')}\"\n",
        "experiment_name = \"inference-test\"\n",
        "inference_component: CommandComponent = get_latest_asset(\n",
        "    ml_client.components,\n",
        "    name=\"workshop_aurora_inference\",\n",
        ")\n",
        "test_inference_command = Command(\n",
        "    component=inference_component,\n",
        "    display_name=display_name,\n",
        "    experiment_name=experiment_name,\n",
        "    compute=\"duncanmartyn-gpu\",  # CLUSTER_NAME\n",
        "    inputs={\n",
        "        \"model\": Input(type=\"custom_model\", path=MODEL_NAME, mode=\"ro_mount\"),\n",
        "        \"data\": Input(type=\"uri_folder\", path=DATA_NAME, mode=\"ro_mount\"),\n",
        "        \"start_datetime\": \"2026-01-01T00:00:00\",  # initial state timestamp\n",
        "        \"steps\": 4,  # number of autoregressive inference steps to perform\n",
        "        \"mode\": \"test\",  # enable test mode to use synthetic data\n",
        "    },\n",
        "    outputs={\n",
        "        \"predictions\": Output(\n",
        "            type=\"uri_file\",\n",
        "            path=f\"azureml://datastores/${{{{default_datastore}}}}/paths/aurora-workshop/output/{experiment_name}/{display_name}/predictions.nc\",\n",
        "            mode=\"rw_mount\",\n",
        "        ),\n",
        "    },\n",
        "    limits=CommandJobLimits(timeout=7200),\n",
        "    distribution=PyTorchDistribution(process_count_per_instance=1),\n",
        "    environment=inference_component.environment,\n",
        ")\n",
        "\n",
        "print(\"Submitting test inference job:\", display_name)\n",
        "test_inference_job = ml_client.jobs.create_or_update(test_inference_command)\n",
        "\n",
        "print(\"Streaming logs:\")\n",
        "ml_client.jobs.stream(test_inference_job.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edeb88d3",
      "metadata": {},
      "source": [
        "## ERA5 inference job\n",
        "\n",
        "Here, we'll run an inference job using pre-loaded ERA5 data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50f1d443",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Submitting ERA5 inference job: duncan-20260116-114901\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFileJobOutput'> and will be ignored\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Streaming logs:\n",
            "RunId: gifted_island_lj35lhp1zm\n",
            "Web View: https://ml.azure.com/runs/gifted_island_lj35lhp1zm?wsid=/subscriptions/62118f5c-be37-400f-9f20-a8b77a2a7877/resourcegroups/data-science-team-rg/workspaces/data-science-team-workspace\n",
            "\n",
            "Execution Summary\n",
            "=================\n",
            "RunId: gifted_island_lj35lhp1zm\n",
            "Web View: https://ml.azure.com/runs/gifted_island_lj35lhp1zm?wsid=/subscriptions/62118f5c-be37-400f-9f20-a8b77a2a7877/resourcegroups/data-science-team-rg/workspaces/data-science-team-workspace\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# NOTE: verify inference works with amended (16, 32) dummy data and .detach\n",
        "display_name = f\"{PARTICIPANT_ID}-{datetime.now(UTC).strftime('%Y%m%d-%H%M%S')}\"\n",
        "experiment_name = \"inference-era5\"\n",
        "era5_inference_command = Command(\n",
        "    component=inference_component,\n",
        "    display_name=display_name,\n",
        "    experiment_name=experiment_name,\n",
        "    compute=\"duncanmartyn-gpu\",  # CLUSTER_NAME\n",
        "    inputs={\n",
        "        \"model\": Input(type=\"custom_model\", path=MODEL_NAME, mode=\"ro_mount\"),\n",
        "        \"data\": Input(type=\"uri_folder\", path=DATA_NAME, mode=\"ro_mount\"),\n",
        "        # below timestamp and that -6 hours must exist in the data\n",
        "        \"start_datetime\": \"2025-01-01T06:00:00\",\n",
        "        \"steps\": 4,\n",
        "        \"mode\": \"eval\",  # enable eval mode to use ERA5\n",
        "    },\n",
        "    outputs={\n",
        "        \"predictions\": Output(\n",
        "            type=\"uri_file\",\n",
        "            path=f\"azureml://datastores/${{{{default_datastore}}}}/paths/aurora-workshop/output/{experiment_name}/{display_name}/predictions.nc\",\n",
        "            mode=\"rw_mount\",\n",
        "        ),\n",
        "    },\n",
        "    limits=CommandJobLimits(timeout=7200),\n",
        "    distribution=PyTorchDistribution(process_count_per_instance=1),\n",
        "    environment=inference_component.environment,\n",
        ")\n",
        "\n",
        "print(\"Submitting ERA5 inference job:\", display_name)\n",
        "era5_inference_job = ml_client.jobs.create_or_update(era5_inference_command)\n",
        "\n",
        "print(\"Streaming logs:\")\n",
        "ml_client.jobs.stream(era5_inference_job.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2854535a",
      "metadata": {},
      "source": [
        "## Fine-tuning jobs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bc2706c",
      "metadata": {},
      "source": [
        "First, load the fine-tuning configs defined in YAML into a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "0fb4a03c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'{\"type\": \"short\", \"steps\": 5, \"mode\": \"test\", \"learning_rate\": \"3e-5\", \"aurora_config\": {\"autocast\": true}}'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with Path(\"finetune_configs.yaml\").open(\"r\") as f:\n",
        "    finetune_configs = yaml.safe_load(f)\n",
        "json.dumps(finetune_configs.get(\"test_short_lead\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f11abac2",
      "metadata": {},
      "source": [
        "Then, specifying the name of a config defined in YAML, run the job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83bc372b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Submitting fine-tuning job: name=duncan-20260116-151135, config=eval_short_lead\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFileJobOutput'> and will be ignored\n",
            "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFileJobOutput'> and will be ignored\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Streaming logs:\n",
            "RunId: sad_horse_nmz0l1l3zk\n",
            "Web View: https://ml.azure.com/runs/sad_horse_nmz0l1l3zk?wsid=/subscriptions/62118f5c-be37-400f-9f20-a8b77a2a7877/resourcegroups/data-science-team-rg/workspaces/data-science-team-workspace\n",
            "\n",
            "Execution Summary\n",
            "=================\n",
            "RunId: sad_horse_nmz0l1l3zk\n",
            "Web View: https://ml.azure.com/runs/sad_horse_nmz0l1l3zk?wsid=/subscriptions/62118f5c-be37-400f-9f20-a8b77a2a7877/resourcegroups/data-science-team-rg/workspaces/data-science-team-workspace\n",
            "\n"
          ]
        }
      ],
      "source": [
        "config_name = input(\"Enter a config name e.g. test_short_lead\").strip()\n",
        "display_name = f\"{PARTICIPANT_ID}-{datetime.now(UTC).strftime('%Y%m%d-%H%M%S')}\"\n",
        "experiment_name = f\"finetuning-{config_name}\"\n",
        "train_component: CommandComponent = get_latest_asset(\n",
        "    ml_client.components,\n",
        "    name=\"workshop_aurora_finetuning\",\n",
        ")\n",
        "train_command = Command(\n",
        "    component=train_component,\n",
        "    display_name=display_name,\n",
        "    experiment_name=experiment_name,\n",
        "    compute=\"duncanmartyn-gpu\",  # CLUSTER_NAME\n",
        "    inputs={\n",
        "        \"model\": Input(type=\"custom_model\", path=MODEL_NAME, mode=\"ro_mount\"),\n",
        "        \"data\": Input(type=\"uri_folder\", path=DATA_NAME, mode=\"ro_mount\"),\n",
        "        # below timestamp and that -6 hours must exist in the data\n",
        "        \"start_datetime\": \"2025-01-01T06:00:00\",\n",
        "        \"config\": json.dumps(finetune_configs.get(config_name)),\n",
        "    },\n",
        "    outputs={\n",
        "        \"loss\": Output(\n",
        "            type=\"uri_file\",\n",
        "            path=f\"azureml://datastores/${{{{default_datastore}}}}/paths/{experiment_name}/{display_name}/loss.npy\",\n",
        "            mode=\"upload\",\n",
        "        ),\n",
        "        \"prediction\": Output(\n",
        "            type=\"uri_file\",\n",
        "            path=f\"azureml://datastores/${{{{default_datastore}}}}/paths/{experiment_name}/{display_name}/prediction.nc\",\n",
        "            mode=\"rw_mount\",\n",
        "        ),\n",
        "    },\n",
        "    limits=CommandJobLimits(timeout=7200),\n",
        "    distribution=PyTorchDistribution(process_count_per_instance=1),\n",
        "    environment=train_component.environment,\n",
        ")\n",
        "\n",
        "print(f\"Submitting fine-tuning job: name={display_name}, config={config_name}\")\n",
        "train_job = ml_client.jobs.create_or_update(train_command)\n",
        "\n",
        "print(\"Streaming logs:\")\n",
        "ml_client.jobs.stream(train_job.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce29e062",
      "metadata": {},
      "source": [
        "## Plotting and evaluating fine-tuning results\n",
        "\n",
        "Here, we'll visualise the products of inference with the pre-trained and fine-tuned (on ERA5) Aurora side-by-side.\n",
        "\n",
        "First, download inference and fine-tuning job outputs and artefacts (logs etc.). This requires the jobs to have completed in a successful state. This, and success in downloading the outputs is verified here.\n",
        "\n",
        "The following new directories and files tagged with * will be created:\n",
        "```md\n",
        "aurora-introductory-workshop/\n",
        "└── notebooks/\n",
        "    └── *outputs/\n",
        "        ├── *inference/\n",
        "        |   ├── *artifacts/: log files for the job, also visible in the job's \"Outputs + logs\" tab in the Studio UI.\n",
        "        |   └── *named-outputs/\n",
        "        |       └── *predictions/\n",
        "        |           └── *predictions.nc: forecasts generated in inference with the pre-trained model and ERA5 data.\n",
        "        └── *training\n",
        "            ├── *artifacts/: log files for the job, also visible in the job's \"Outputs + logs\" tab in the Studio UI.\n",
        "            └── *named-outputs/\n",
        "                ├── *loss/\n",
        "                |   └── *loss.npy: loss history (loss values at each step) of fine-tuning.\n",
        "                └── *prediction/\n",
        "                    └── *prediction.nc: last forecast generated in inference with the fine-tuned model and ERA5 data.\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "617e8f04",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = Path(\"outputs\")\n",
        "data_dir.mkdir(exist_ok=True)\n",
        "inference_out_dir = data_dir / \"inference\"\n",
        "inference_out_dir.mkdir(exist_ok=True)\n",
        "training_out_dir = data_dir / \"training\"\n",
        "training_out_dir.mkdir(exist_ok=True)\n",
        "\n",
        "try:\n",
        "    print(\"Downloading inference job outputs for job:\", era5_inference_job.display_name)\n",
        "    ml_client.jobs.download(\n",
        "        name=era5_inference_job.name,\n",
        "        download_path=inference_out_dir,\n",
        "        all=True,\n",
        "    )\n",
        "    print(\"Downloaded inference job outputs to:\", inference_out_dir)\n",
        "\n",
        "    print(\"Downloading training job outputs for job:\", era5_train_job.display_name)\n",
        "    ml_client.jobs.download(\n",
        "        name=era5_train_job.name,\n",
        "        download_path=training_out_dir,\n",
        "        all=True,\n",
        "    )\n",
        "    print(\"Downloaded fine-tuning outputs to:\", training_out_dir)\n",
        "\n",
        "except (JobException, MlException) as e:\n",
        "    print(\"Failed to download job outputs and logs, has the job succeeded?\", e)\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a318bdf7",
      "metadata": {},
      "source": [
        "Second, load job outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "292de99d",
      "metadata": {},
      "outputs": [],
      "source": [
        "inference_ds_path = inference_out_dir / \"named-outputs/predictions/predictions.nc\"\n",
        "finetune_ds_path = training_out_dir / \"named-outputs/prediction/prediction.nc\"\n",
        "loss_arr_path = training_out_dir / \"named-outputs/loss/loss.npy\"\n",
        "\n",
        "inference_ds = xr.open_dataset(inference_ds_path)\n",
        "finetune_ds = xr.open_dataset(finetune_ds_path)\n",
        "loss_arr = np.load(loss_arr_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d34b0121",
      "metadata": {},
      "outputs": [],
      "source": [
        "# plotting code here"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "aurora-introductory-workshop",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
