{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e69381ca-fa9b-4cdb-9bbf-971650c14107",
      "metadata": {},
      "source": [
        "# Aurora inference & fine-tuning in Azure ML\n",
        "\n",
        "This notebook explains the workflow and submits an Azure Machine Learning (AML) job that runs **Microsoft Aurora** on a GPU cluster.\n",
        "\n",
        "## Files used in this workshop\n",
        "\n",
        "We use the following components:\n",
        "\n",
        "1. `notebooks/0_aurora_workshop.ipynb` *(this notebook)* – explains the workflow and submits jobs to AML:\n",
        "   - <mark>Run this notebook with a CPU Compute Instance using the \"Python 3.10 - SDK v2\" kernel</mark>\n",
        "2. `setup/components/inference` - contains Aurora inference logic:\n",
        "   - `main.py`: a script with a CLI interface for running a simple inference loop.\n",
        "   - `component.py`: AML component definition.\n",
        "3. `setup/components/training` - contains Aurora fine-tuning logic:\n",
        "   - `main.py`: a script with a CLI interface for running a simple fine-tuning loop.\n",
        "   - `component.py`: AML component definition.\n",
        "4. `setup/components/common/utils.py` - contains Aurora helper logic, including:\n",
        "   - Loading model checkpoints in train or eval mode.\n",
        "   - Loading data on disk into `aurora.Batch` objects for inference and fine-tuning.\n",
        "   - Converting `aurora.Batch` objects into `xarray.Dataset` objects for analysis and writing of data.\n",
        "\n",
        "**NOTE**: inference and fine-tuning scripts in `setup/components/*/main.py` will work in local and remote environments provided the hardware and dependencies required to run Aurora are present in each. AML component definitions in `setup/components/*/components.py` serve only to deploy and make these scripts executable in AML."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "338243af",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import sys\n",
        "from datetime import UTC, datetime\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "import yaml\n",
        "from azure.ai.ml import Input, Output, PyTorchDistribution\n",
        "from azure.ai.ml.entities import Command, CommandComponent, CommandJobLimits, Model\n",
        "from azure.ai.ml.exceptions import JobException, MlException\n",
        "\n",
        "sys.path.insert(0, str(Path.cwd().parent.resolve()))\n",
        "from setup.common.utils import create_mlclient, get_latest_asset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d553f539",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, duncanmartyn!\n"
          ]
        }
      ],
      "source": [
        "PARTICIPANT_ID = input(\"Enter a participant ID e.g. saadatali\").strip()\n",
        "print(f\"Hello, {PARTICIPANT_ID}!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c30758e",
      "metadata": {},
      "source": [
        "Create an `azure.ai.ml.MLClient` object to interact with the workspace and, with this, retrieve the compute cluster, model, and data required to run jobs.\n",
        "\n",
        "**NOTE**: the `local` parameter expects a boolean argument that decides what environment variables to look for when configuring the `MLClient` object. `True` will look for environment variables set in a local `.env` file in the project root, `False` will look for environment variables automatically set in Azure Machine Learning Compute Instances. See `setup/common/utils.py` for more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "7a5afeef",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Overriding of current TracerProvider is not allowed\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Overriding of current LoggerProvider is not allowed\n",
            "Overriding of current MeterProvider is not allowed\n",
            "Attempting to instrument while already instrumented\n",
            "Attempting to instrument while already instrumented\n",
            "Attempting to instrument while already instrumented\n",
            "Attempting to instrument while already instrumented\n",
            "Attempting to instrument while already instrumented\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connected to workspace: sub=62118f5c-be37-400f-9f20-a8b77a2a7877, rg=data-science-team-rg, workspace=data-science-team-workspace\n",
            "Using assets: cluster=test-cluster, model=azureml:aurora-0p25-pretrained:2, data=azureml:workshop-test-asset:7\n"
          ]
        }
      ],
      "source": [
        "ml_client = create_mlclient(local=True)\n",
        "print(\n",
        "    f\"Connected to workspace: sub={ml_client.subscription_id}, \"\n",
        "    f\"rg={ml_client.resource_group_name}, workspace={ml_client.workspace_name}\",\n",
        ")\n",
        "OUTFILE_TEMPLATE = f\"azureml://datastores/${{{{{{{{default_datastore}}}}}}}}/paths/aurora-workshop/{PARTICIPANT_ID}/{{experiment_name}}/{{display_name}}/{{filename}}\"\n",
        "\n",
        "# get the name of the first AML compute cluster (type=\"amlcompute\") in the workspace\n",
        "CLUSTER_NAME = next(iter(ml_client.compute.list(compute_type=\"amlcompute\"))).name\n",
        "\n",
        "# get the latest pre-trained Aurora 0.25 model registered in the workspace\n",
        "model = get_latest_asset(ml_client.models, name=\"aurora-0p25-pretrained\")\n",
        "MODEL_NAME = f\"azureml:{model.name}:{model.version}\"\n",
        "\n",
        "# get the latest ERA5 subset data asset registered in the workspace\n",
        "data = get_latest_asset(ml_client.data, name=\"workshop-test-asset\")\n",
        "DATA_NAME = f\"azureml:{data.name}:{data.version}\"\n",
        "\n",
        "print(f\"Using assets: cluster={CLUSTER_NAME}, model={MODEL_NAME}, data={DATA_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "767c8213",
      "metadata": {},
      "source": [
        "## Inference jobs\n",
        "\n",
        "Here, we'll run inference jobs using different data:\n",
        "- Generated synthetic test data comprising a low resolution tensor of random float values.\n",
        "- Real, pre-loaded ERA5 data over the 2025-01-01T00 to 2025-01-31T18 period.\n",
        "\n",
        "First, load the fine-tuning configs defined in YAML into a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "af3a20b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "with Path(\"inference_configs.yaml\").open(\"r\") as f:\n",
        "    inference_configs = yaml.safe_load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff79105d",
      "metadata": {},
      "source": [
        "Then, specifying the name of a config defined in YAML, run the job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47fdfc99",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Submitting inference job: name=duncanmartyn-20260120-104215, config=test\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFileJobOutput'> and will be ignored\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Streaming logs:\n",
            "RunId: cool_candle_clw8q5nnxp\n",
            "Web View: https://ml.azure.com/runs/cool_candle_clw8q5nnxp?wsid=/subscriptions/62118f5c-be37-400f-9f20-a8b77a2a7877/resourcegroups/data-science-team-rg/workspaces/data-science-team-workspace\n",
            "\n",
            "Execution Summary\n",
            "=================\n",
            "RunId: cool_candle_clw8q5nnxp\n",
            "Web View: https://ml.azure.com/runs/cool_candle_clw8q5nnxp?wsid=/subscriptions/62118f5c-be37-400f-9f20-a8b77a2a7877/resourcegroups/data-science-team-rg/workspaces/data-science-team-workspace\n",
            "\n"
          ]
        }
      ],
      "source": [
        "config_name = input(\"Enter a config name e.g. test\").strip()\n",
        "display_name = f\"{PARTICIPANT_ID}-{datetime.now(UTC).strftime('%Y%m%d-%H%M%S')}\"\n",
        "experiment_name = f\"inference-{config_name}\"\n",
        "inference_component: CommandComponent = get_latest_asset(\n",
        "    ml_client.components,\n",
        "    name=\"workshop_aurora_inference\",\n",
        ")\n",
        "inference_command = Command(\n",
        "    component=inference_component,\n",
        "    display_name=display_name,\n",
        "    experiment_name=experiment_name,\n",
        "    compute=\"duncanmartyn-gpu\",  # CLUSTER_NAME\n",
        "    inputs={\n",
        "        \"model\": Input(type=\"custom_model\", path=MODEL_NAME, mode=\"ro_mount\"),\n",
        "        \"data\": Input(type=\"uri_folder\", path=DATA_NAME, mode=\"ro_mount\"),\n",
        "        # initial state timestamp below and that -6 hours must exist in the data\n",
        "        \"start_datetime\": \"2025-01-01T06:00:00\",\n",
        "        \"config\": json.dumps(inference_configs.get(config_name)),\n",
        "    },\n",
        "    outputs={\n",
        "        \"predictions\": Output(\n",
        "            type=\"uri_file\",\n",
        "            path=OUTFILE_TEMPLATE.format(\n",
        "                experiment_name=experiment_name,\n",
        "                display_name=display_name,\n",
        "                filename=\"predictions.nc\",\n",
        "            ),\n",
        "            mode=\"rw_mount\",\n",
        "        ),\n",
        "    },\n",
        "    limits=CommandJobLimits(timeout=7200),\n",
        "    distribution=PyTorchDistribution(process_count_per_instance=1),\n",
        "    environment=inference_component.environment,\n",
        ")\n",
        "\n",
        "print(f\"Submitting inference job: name={display_name}, config={config_name}\")\n",
        "inference_job = ml_client.jobs.create_or_update(inference_command)\n",
        "\n",
        "print(\"Streaming logs:\")\n",
        "ml_client.jobs.stream(inference_job.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2854535a",
      "metadata": {},
      "source": [
        "## Fine-tuning jobs\n",
        "\n",
        "Here, we'll run fine-tuning jobs using different data:\n",
        "- Generated synthetic test data comprising a low resolution tensor of random float values.\n",
        "- Real, pre-loaded ERA5 data over the 2025-01-01T00 to 2025-01-31T18 period.\n",
        "\n",
        "First, load the fine-tuning configs defined in YAML into a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "0fb4a03c",
      "metadata": {},
      "outputs": [],
      "source": [
        "with Path(\"finetune_configs.yaml\").open(\"r\") as f:\n",
        "    finetune_configs = yaml.safe_load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f11abac2",
      "metadata": {},
      "source": [
        "Then, specifying the name of a config defined in YAML, run the job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "83bc372b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Submitting fine-tuning job: name=duncanmartyn-20260120-143343, config=eval_add_variable\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFileJobOutput'> and will be ignored\n",
            "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFileJobOutput'> and will be ignored\n",
            "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFileJobOutput'> and will be ignored\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Streaming logs:\n",
            "RunId: khaki_camera_y12wgc875p\n",
            "Web View: https://ml.azure.com/runs/khaki_camera_y12wgc875p?wsid=/subscriptions/62118f5c-be37-400f-9f20-a8b77a2a7877/resourcegroups/data-science-team-rg/workspaces/data-science-team-workspace\n",
            "\n",
            "Execution Summary\n",
            "=================\n",
            "RunId: khaki_camera_y12wgc875p\n",
            "Web View: https://ml.azure.com/runs/khaki_camera_y12wgc875p?wsid=/subscriptions/62118f5c-be37-400f-9f20-a8b77a2a7877/resourcegroups/data-science-team-rg/workspaces/data-science-team-workspace\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# config_name = input(\"Enter a config name e.g. test_short_lead\").strip()\n",
        "config_name = \"eval_add_variable\"\n",
        "display_name = f\"{PARTICIPANT_ID}-{datetime.now(UTC).strftime('%Y%m%d-%H%M%S')}\"\n",
        "experiment_name = f\"finetuning-{config_name}\"\n",
        "train_component: CommandComponent = get_latest_asset(\n",
        "    ml_client.components,\n",
        "    name=\"workshop_aurora_finetuning\",\n",
        ")\n",
        "train_command = Command(\n",
        "    component=train_component,\n",
        "    display_name=display_name,\n",
        "    experiment_name=experiment_name,\n",
        "    compute=\"duncanmartyn-gpu\",  # CLUSTER_NAME\n",
        "    inputs={\n",
        "        \"model\": Input(type=\"custom_model\", path=MODEL_NAME, mode=\"ro_mount\"),\n",
        "        \"data\": Input(type=\"uri_folder\", path=DATA_NAME, mode=\"ro_mount\"),\n",
        "        # below timestamp and that -6 hours must exist in the data\n",
        "        \"start_datetime\": \"2025-01-01T06:00:00\",\n",
        "        \"config\": json.dumps(finetune_configs.get(config_name)),\n",
        "    },\n",
        "    outputs={\n",
        "        \"loss\": Output(\n",
        "            type=\"uri_file\",\n",
        "            path=OUTFILE_TEMPLATE.format(\n",
        "                experiment_name=experiment_name,\n",
        "                display_name=display_name,\n",
        "                filename=\"loss.npy\",\n",
        "            ),\n",
        "            mode=\"upload\",\n",
        "        ),\n",
        "        \"prediction\": Output(\n",
        "            type=\"uri_file\",\n",
        "            path=OUTFILE_TEMPLATE.format(\n",
        "                experiment_name=experiment_name,\n",
        "                display_name=display_name,\n",
        "                filename=\"prediction.nc\",\n",
        "            ),\n",
        "            mode=\"rw_mount\",\n",
        "        ),\n",
        "        \"finetuned\": Output(\n",
        "            type=\"uri_file\",\n",
        "            path=OUTFILE_TEMPLATE.format(\n",
        "                experiment_name=experiment_name,\n",
        "                display_name=display_name,\n",
        "                filename=\"finetuned.ckpt\",\n",
        "            ),\n",
        "            mode=\"rw_mount\",\n",
        "        ),\n",
        "    },\n",
        "    limits=CommandJobLimits(timeout=7200),\n",
        "    distribution=PyTorchDistribution(process_count_per_instance=1),\n",
        "    environment=train_component.environment,\n",
        ")\n",
        "\n",
        "print(f\"Submitting fine-tuning job: name={display_name}, config={config_name}\")\n",
        "train_job = ml_client.jobs.create_or_update(train_command)\n",
        "\n",
        "print(\"Streaming logs:\")\n",
        "ml_client.jobs.stream(train_job.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bce9c08",
      "metadata": {},
      "source": [
        "Next, we register the fine-tuned model using the job output location.\n",
        "\n",
        "NOTE: there are several ways to specify the location of model assets [described in documentation](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-models?view=azureml-api-2&tabs=cli)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d98db971",
      "metadata": {},
      "outputs": [],
      "source": [
        "# register model from job output\n",
        "# azureml://jobs/<job-name>/outputs/<output-name>/paths/<path-to-model-relative-to-the-named-output-location>\n",
        "# azureml:azureml_khaki_camera_y12wgc875p_output_data_finetuned:1\n",
        "model = Model(\n",
        "    name=f\"{display_name}-aurora-finetuned\",\n",
        "    version=\"1\",\n",
        "    path=f\"azureml://jobs/{train_job.name}/outputs/finetuned/paths/finetuned.ckpt\",\n",
        "    description=\"Fine-tuned Aurora model.\",\n",
        "    tags={\n",
        "        \"author\": PARTICIPANT_ID,\n",
        "        \"config_name\": config_name,\n",
        "        \"experiment_name\": experiment_name,\n",
        "        \"job_name\": train_job.name,\n",
        "        \"job_url\": train_job.studio_url,\n",
        "    },\n",
        ")\n",
        "ml_client.models.create_or_update(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9320ea48",
      "metadata": {},
      "outputs": [],
      "source": [
        "# test fine-tuned model inference"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce29e062",
      "metadata": {},
      "source": [
        "## Plotting and evaluating fine-tuning results\n",
        "\n",
        "Here, we'll visualise the products of inference with the pre-trained and fine-tuned (on ERA5) Aurora side-by-side.\n",
        "\n",
        "First, download inference and fine-tuning job outputs and artefacts (logs etc.). This requires the jobs to have completed in a successful state. This, and success in downloading the outputs is verified here.\n",
        "\n",
        "The following new directories and files tagged with * will be created:\n",
        "```md\n",
        "aurora-introductory-workshop/\n",
        "└── notebooks/\n",
        "    └── *outputs/\n",
        "        ├── *inference/\n",
        "        |   ├── *artifacts/: log files for the job, also visible in the job's \"Outputs + logs\" tab in the Studio UI.\n",
        "        |   └── *named-outputs/\n",
        "        |       └── *predictions/\n",
        "        |           └── *predictions.nc: forecasts generated in inference with the pre-trained model and ERA5 data.\n",
        "        └── *training\n",
        "            ├── *artifacts/: log files for the job, also visible in the job's \"Outputs + logs\" tab in the Studio UI.\n",
        "            └── *named-outputs/\n",
        "                ├── *loss/\n",
        "                |   └── *loss.npy: loss history (loss values at each step) of fine-tuning.\n",
        "                └── *prediction/\n",
        "                    └── *prediction.nc: last forecast generated in inference with the fine-tuned model and ERA5 data.\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "617e8f04",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = Path(\"outputs\")\n",
        "data_dir.mkdir(exist_ok=True)\n",
        "inference_out_dir = data_dir / \"inference\"\n",
        "inference_out_dir.mkdir(exist_ok=True)\n",
        "training_out_dir = data_dir / \"training\"\n",
        "training_out_dir.mkdir(exist_ok=True)\n",
        "\n",
        "try:\n",
        "    print(\"Downloading inference job outputs for job:\", inference_job.display_name)\n",
        "    ml_client.jobs.download(\n",
        "        name=inference_job.name,\n",
        "        download_path=inference_out_dir,\n",
        "        all=True,\n",
        "    )\n",
        "    print(\"Downloaded inference job outputs to:\", inference_out_dir)\n",
        "\n",
        "    print(\"Downloading training job outputs for job:\", train_job.display_name)\n",
        "    ml_client.jobs.download(\n",
        "        name=train_job.name,\n",
        "        download_path=training_out_dir,\n",
        "        all=True,\n",
        "    )\n",
        "    print(\"Downloaded fine-tuning outputs to:\", training_out_dir)\n",
        "\n",
        "except (JobException, MlException) as e:\n",
        "    print(\"Failed to download job outputs and logs, verify the job has succeeded.\", e)\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a318bdf7",
      "metadata": {},
      "source": [
        "Second, load job outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "292de99d",
      "metadata": {},
      "outputs": [],
      "source": [
        "inference_ds_path = inference_out_dir / \"named-outputs/predictions/predictions.nc\"\n",
        "finetune_ds_path = training_out_dir / \"named-outputs/prediction/prediction.nc\"\n",
        "loss_arr_path = training_out_dir / \"named-outputs/loss/loss.npy\"\n",
        "\n",
        "inference_ds = xr.open_dataset(inference_ds_path)\n",
        "finetune_ds = xr.open_dataset(finetune_ds_path)\n",
        "loss_arr = np.load(loss_arr_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d6622c1",
      "metadata": {},
      "source": [
        "Finally, plot the downloaded data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d34b0121",
      "metadata": {},
      "outputs": [],
      "source": [
        "# plotting code here\n",
        "import matplotlib.pyplot as plt"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "aurora-introductory-workshop",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
