{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e69381ca-fa9b-4cdb-9bbf-971650c14107",
      "metadata": {},
      "source": [
        "# Aurora inference & fine-tuning in Azure ML\n",
        "\n",
        "This notebook explains the workflow and submits an Azure Machine Learning (AML) job that runs **Microsoft Aurora** on a GPU cluster.\n",
        "\n",
        "## Files used in this workshop\n",
        "\n",
        "We use the following components:\n",
        "\n",
        "1. `notebooks/0_aurora_workshop.ipynb` *(this notebook)* – explains the workflow and submits jobs to AML:\n",
        "   - <mark>Run this notebook with a CPU Compute Instance using the \"Python 3.10 - SDK v2\" kernel</mark>\n",
        "2. `setup/components/inference` - contains Aurora inference logic:\n",
        "   - `main.py`: a script with a CLI interface for running a simple inference loop.\n",
        "   - `component.py`: AML component definition.\n",
        "3. `setup/components/training` - contains Aurora fine-tuning logic:\n",
        "   - `main.py`: a script with a CLI interface for running a simple fine-tuning loop.\n",
        "   - `component.py`: AML component definition.\n",
        "4. `setup/components/common/utils.py` - contains Aurora helper logic, including:\n",
        "   - Loading model checkpoints in train or eval mode.\n",
        "   - Loading data on disk into `aurora.Batch` objects for inference and fine-tuning.\n",
        "   - Converting `aurora.Batch` objects into `xarray.Dataset` objects for analysis and writing of data.\n",
        "\n",
        "**NOTE**: inference and fine-tuning scripts in `setup/components/*/main.py` will work in local and remote environments provided the hardware and dependencies required to run Aurora are present in each. AML component definitions in `setup/components/*/components.py` serve only to deploy and make these scripts executable in AML. Notebooks and common code in `setup/notebooks` and `setup/common`, respectively, are largely for workspace setup including data asset and pre-trained model download and registration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8488f359",
      "metadata": {},
      "outputs": [],
      "source": [
        "# use Python 3.10 - SDK v2 kernel to avoid numpy / xarray version issues\n",
        "# install necessary dependencies for this notebook and setup.common.utils\n",
        "%pip install azure-ai-ml xarray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "338243af",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import sys\n",
        "from datetime import UTC, datetime\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "import yaml\n",
        "from azure.ai.ml import Input, Output, PyTorchDistribution\n",
        "from azure.ai.ml.entities import Command, CommandJobLimits, Model\n",
        "from azure.ai.ml.exceptions import JobException, MlException\n",
        "\n",
        "sys.path.insert(0, str(Path.cwd().parent.resolve()))\n",
        "from setup.common.utils import create_mlclient, get_latest_asset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d553f539",
      "metadata": {},
      "outputs": [],
      "source": [
        "PARTICIPANT_ID = input(\"Enter your firstnamelastname format participant ID.\").strip()\n",
        "print(f\"Hello, {PARTICIPANT_ID}!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c30758e",
      "metadata": {},
      "source": [
        "Create an `azure.ai.ml.MLClient` object to interact with the workspace and, with this, retrieve the compute cluster, model, and data required to run jobs. This assumes there is one cluster in the workspace and that it is GPU enabled.\n",
        "\n",
        "**NOTE**: the `local` parameter expects a boolean argument that decides what environment variables to look for when configuring the `MLClient` object. `True` will look for environment variables set in a local `.env` file in the project root, `False` will look for environment variables automatically set in Azure Machine Learning Compute Instances. See `setup/common/utils.py` for more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a5afeef",
      "metadata": {},
      "outputs": [],
      "source": [
        "ml_client = create_mlclient(local=True)\n",
        "print(\n",
        "    f\"Connected to workspace: sub={ml_client.subscription_id}, \"\n",
        "    f\"rg={ml_client.resource_group_name}, workspace={ml_client.workspace_name}\",\n",
        ")\n",
        "OUTFILE_TEMPLATE = f\"azureml://datastores/${{{{{{{{default_datastore}}}}}}}}/paths/aurora-workshop/{PARTICIPANT_ID}/{{experiment_name}}/{{display_name}}/{{filename}}\"\n",
        "\n",
        "# get the name of the first AML compute cluster (type=\"amlcompute\") in the workspace\n",
        "CLUSTER_NAME = next(iter(ml_client.compute.list(compute_type=\"amlcompute\"))).name\n",
        "\n",
        "# get the latest pre-trained Aurora 0.25 model registered in the workspace\n",
        "model = get_latest_asset(ml_client.models, name=\"aurora-0p25-pretrained\")\n",
        "MODEL_NAME = f\"azureml:{model.name}:{model.version}\"\n",
        "\n",
        "# get the latest ERA5 subset data asset registered in the workspace\n",
        "data = get_latest_asset(ml_client.data, name=\"gcp-era5-arco\")\n",
        "DATA_NAME = f\"azureml:{data.name}:{data.version}\"\n",
        "\n",
        "print(f\"Using assets: cluster={CLUSTER_NAME}, model={MODEL_NAME}, data={DATA_NAME}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "767c8213",
      "metadata": {},
      "source": [
        "## Inference jobs\n",
        "\n",
        "Here, we'll run inference and evaluation jobs using different data:\n",
        "- Generated synthetic test data comprising a low resolution tensor of random float values.\n",
        "- Real, pre-loaded ERA5 data over the 2025-01-01T00 to 2025-01-31T18 period.\n",
        "\n",
        "After inference, the final prediction is compared to its corresponding ground truth. We calculate global difference and RMSE, logging the plot and value, respectively. Both are logged with [MLflow](https://mlflow.org/), an open-source machine learning lifecycle framework [integrated in AML](https://learn.microsoft.com/en-us/azure/machine-learning/concept-mlflow?view=azureml-api-2).\n",
        "\n",
        "First, load the fine-tuning configs defined in YAML into a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af3a20b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "with Path(\"inference_configs.yaml\").open(\"r\") as f:\n",
        "    inference_configs = yaml.safe_load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff79105d",
      "metadata": {},
      "source": [
        "Then, specifying the name of a config defined in YAML, run the job.\n",
        "\n",
        "**NOTE:** for the definition and logic of the Command Component used in this job, see either `setup/components/inference` or the registered component in AML."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47fdfc99",
      "metadata": {},
      "outputs": [],
      "source": [
        "config_name = input(\"Enter a config name e.g. test\").strip()\n",
        "if not (cfg := inference_configs.get(config_name)):\n",
        "    msg = f\"Config not found: name={config_name}\"\n",
        "    raise ValueError(msg)\n",
        "\n",
        "display_name = f\"{PARTICIPANT_ID}-{datetime.now(UTC).strftime('%Y%m%d-%H%M%S')}\"\n",
        "experiment_name = f\"inference-{config_name}\"\n",
        "inference_component = get_latest_asset(ml_client.components, name=\"aurora_inference\")\n",
        "\n",
        "inference_command = Command(\n",
        "    component=inference_component,\n",
        "    display_name=display_name,\n",
        "    experiment_name=experiment_name,\n",
        "    compute=CLUSTER_NAME,\n",
        "    inputs={\n",
        "        \"model\": Input(type=\"custom_model\", path=MODEL_NAME, mode=\"ro_mount\"),\n",
        "        \"data\": Input(type=\"uri_folder\", path=DATA_NAME, mode=\"ro_mount\"),\n",
        "        # initial state timestamp below and that -6 hours must exist in the data\n",
        "        \"start_datetime\": \"2025-01-01T06:00:00\",\n",
        "        \"config\": json.dumps(cfg),\n",
        "    },\n",
        "    outputs={\n",
        "        \"predictions\": Output(\n",
        "            type=\"uri_file\",\n",
        "            path=OUTFILE_TEMPLATE.format(\n",
        "                experiment_name=experiment_name,\n",
        "                display_name=display_name,\n",
        "                filename=\"predictions.nc\",\n",
        "            ),\n",
        "            mode=\"rw_mount\",\n",
        "        ),\n",
        "    },\n",
        "    limits=CommandJobLimits(timeout=7200),\n",
        "    distribution=PyTorchDistribution(process_count_per_instance=1),\n",
        "    environment=inference_component.environment,\n",
        ")\n",
        "\n",
        "print(f\"Submitting inference job: name={display_name}, config={config_name}\")\n",
        "inference_job = ml_client.jobs.create_or_update(inference_command)\n",
        "\n",
        "print(\"Streaming logs:\")\n",
        "ml_client.jobs.stream(inference_job.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2854535a",
      "metadata": {},
      "source": [
        "## Fine-tuning jobs\n",
        "\n",
        "Here, we'll run fine-tuning jobs using different data:\n",
        "- Generated synthetic test data comprising a low resolution tensor of random float values.\n",
        "- Real, pre-loaded ERA5 data over the 2025-01-01T00 to 2025-01-31T18 period.\n",
        "\n",
        "First, load the fine-tuning configs defined in YAML into a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fb4a03c",
      "metadata": {},
      "outputs": [],
      "source": [
        "with Path(\"finetune_configs.yaml\").open(\"r\") as f:\n",
        "    finetune_configs = yaml.safe_load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f11abac2",
      "metadata": {},
      "source": [
        "Then, specifying the name of a config defined in YAML, run the job.\n",
        "\n",
        "For the definition and logic of the Command Component used in this job, see either `setup/components/training` or the registered component in AML."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83bc372b",
      "metadata": {},
      "outputs": [],
      "source": [
        "config_name = input(\"Enter a config name e.g. test_short_lead\").strip()\n",
        "if not (cfg := finetune_configs.get(config_name)):\n",
        "    msg = f\"Config not found: name={config_name}\"\n",
        "    raise ValueError(msg)\n",
        "\n",
        "display_name = f\"{PARTICIPANT_ID}-{datetime.now(UTC).strftime('%Y%m%d-%H%M%S')}\"\n",
        "experiment_name = f\"finetuning-{config_name}\"\n",
        "train_component = get_latest_asset(ml_client.components, name=\"aurora_finetuning\")\n",
        "\n",
        "train_command = Command(\n",
        "    component=train_component,\n",
        "    display_name=display_name,\n",
        "    experiment_name=experiment_name,\n",
        "    compute=CLUSTER_NAME,\n",
        "    inputs={\n",
        "        \"model\": Input(type=\"custom_model\", path=MODEL_NAME, mode=\"ro_mount\"),\n",
        "        \"data\": Input(type=\"uri_folder\", path=DATA_NAME, mode=\"ro_mount\"),\n",
        "        # below timestamp and that -6 hours must exist in the data\n",
        "        \"start_datetime\": \"2025-01-01T06:00:00\",\n",
        "        # below timestamp only possibly used as a training target\n",
        "        \"end_datetime\": \"2025-01-31T23:00:00\",\n",
        "        \"config\": json.dumps(cfg),\n",
        "    },\n",
        "    outputs={\n",
        "        \"loss\": Output(\n",
        "            type=\"uri_file\",\n",
        "            path=OUTFILE_TEMPLATE.format(\n",
        "                experiment_name=experiment_name,\n",
        "                display_name=display_name,\n",
        "                filename=\"loss.npy\",\n",
        "            ),\n",
        "            mode=\"upload\",\n",
        "        ),\n",
        "        \"prediction\": Output(\n",
        "            type=\"uri_file\",\n",
        "            path=OUTFILE_TEMPLATE.format(\n",
        "                experiment_name=experiment_name,\n",
        "                display_name=display_name,\n",
        "                filename=\"prediction.nc\",\n",
        "            ),\n",
        "            mode=\"rw_mount\",\n",
        "        ),\n",
        "        \"finetuned\": Output(\n",
        "            type=\"uri_file\",\n",
        "            path=OUTFILE_TEMPLATE.format(\n",
        "                experiment_name=experiment_name,\n",
        "                display_name=display_name,\n",
        "                filename=\"finetuned.ckpt\",\n",
        "            ),\n",
        "            mode=\"rw_mount\",\n",
        "        ),\n",
        "    },\n",
        "    limits=CommandJobLimits(timeout=7200),\n",
        "    distribution=PyTorchDistribution(process_count_per_instance=1),\n",
        "    environment=train_component.environment,\n",
        ")\n",
        "\n",
        "print(f\"Submitting fine-tuning job: name={display_name}, config={config_name}\")\n",
        "train_job = ml_client.jobs.create_or_update(train_command)\n",
        "\n",
        "print(\"Streaming logs:\")\n",
        "ml_client.jobs.stream(train_job.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bce9c08",
      "metadata": {},
      "source": [
        "Next, we register the fine-tuned model in AML using the job output location. This enables us to easily track, version, use, and deploy models.\n",
        "\n",
        "**NOTE:** there are several ways to specify the location of model assets [described in documentation](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-models?view=azureml-api-2&tabs=cli)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d98db971",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = Model(\n",
        "    name=f\"{display_name}-aurora-finetuned\",\n",
        "    version=\"1\",\n",
        "    path=f\"azureml://jobs/{train_job.name}/outputs/finetuned/paths/\",\n",
        "    description=\"Fine-tuned Aurora model.\",\n",
        "    tags={\"author\": PARTICIPANT_ID, \"config_name\": config_name},\n",
        ")\n",
        "ml_client.models.create_or_update(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce29e062",
      "metadata": {},
      "source": [
        "## [Optional] Plotting and evaluating fine-tuning results\n",
        "\n",
        "Here, we'll download the products of inference and fine-tuning from their respective AML jobs.\n",
        "\n",
        "First, download inference and fine-tuning job outputs and artefacts (logs etc.). This requires the jobs to have completed in a successful state.\n",
        "\n",
        "The following new directories and files tagged with * will be created:\n",
        "```md\n",
        "aurora-introductory-workshop/\n",
        "└── notebooks/\n",
        "    └── *outputs/\n",
        "        ├── *inference/\n",
        "        |   ├── *artifacts/: log files for the job, also visible in the job's \"Outputs + logs\" tab in the Studio UI.\n",
        "        |   └── *named-outputs/\n",
        "        |       └── *predictions/\n",
        "        |           └── *predictions.nc: forecasts generated in inference with the pre-trained model and ERA5 data.\n",
        "        └── *training\n",
        "            ├── *artifacts/: log files for the job, also visible in the job's \"Outputs + logs\" tab in the Studio UI.\n",
        "            └── *named-outputs/\n",
        "                ├── *loss/\n",
        "                |   └── *loss.npy: loss history (loss values at each step) of fine-tuning.\n",
        "                ├── *prediction/\n",
        "                |   └── *prediction.nc: last forecast generated in inference with the fine-tuned model and ERA5 data.\n",
        "                └── *finetuned/\n",
        "                    └── *finetuned.ckpt: fine-tuned model checkpoint.\n",
        "```\n",
        "**NOTE:** download specific outputs by replacing `all=True` with `output_name=<name of output>`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "617e8f04",
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = Path(\"outputs\")\n",
        "data_dir.mkdir(exist_ok=True)\n",
        "inference_out_dir = data_dir / \"inference\"\n",
        "inference_out_dir.mkdir(exist_ok=True)\n",
        "training_out_dir = data_dir / \"training\"\n",
        "training_out_dir.mkdir(exist_ok=True)\n",
        "\n",
        "try:\n",
        "    print(\"Downloading inference job outputs for job:\", inference_job.display_name)\n",
        "    ml_client.jobs.download(\n",
        "        name=inference_job.name,\n",
        "        download_path=inference_out_dir,\n",
        "        all=True,\n",
        "    )\n",
        "    print(\"Downloaded inference job outputs to:\", inference_out_dir)\n",
        "\n",
        "    print(\"Downloading training job outputs for job:\", train_job.display_name)\n",
        "    ml_client.jobs.download(\n",
        "        name=train_job.name,\n",
        "        download_path=training_out_dir,\n",
        "        all=True,\n",
        "    )\n",
        "    print(\"Downloaded fine-tuning outputs to:\", training_out_dir)\n",
        "\n",
        "except (JobException, MlException) as e:\n",
        "    print(\"Failed to download job outputs and logs, verify the job has succeeded.\", e)\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a318bdf7",
      "metadata": {},
      "source": [
        "Second, load job outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "292de99d",
      "metadata": {},
      "outputs": [],
      "source": [
        "inference_ds_path = inference_out_dir / \"named-outputs/predictions/predictions.nc\"\n",
        "finetune_ds_path = training_out_dir / \"named-outputs/prediction/prediction.nc\"\n",
        "loss_arr_path = training_out_dir / \"named-outputs/loss/loss.npy\"\n",
        "\n",
        "inference_ds = xr.open_dataset(inference_ds_path)\n",
        "finetune_ds = xr.open_dataset(finetune_ds_path)\n",
        "loss_arr = np.load(loss_arr_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d6622c1",
      "metadata": {},
      "source": [
        "Finally, plot the downloaded data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d34b0121",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3295009b",
      "metadata": {},
      "source": [
        "## [Optional] Create an Aurora Batch\n",
        "\n",
        "Using the downloaded NetCDFs produced by previous jobs, try to create an `aurora.Batch` object from the data within."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81fd24e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "from aurora import Batch"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "aurora-introductory-workshop",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
