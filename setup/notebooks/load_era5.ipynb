{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Loading ERA5\n",
        "\n",
        "This notebook serves to extract, filter, and load ERA5 data from Google's public ERA5 analysis-ready, cloud-optimised (ARCO) mirror into Azure Blob Storage. Here, ERA5 data for the 1940-01-01 to 2025-12-31 period (continually, if irregularly, updated) at hourly frequency is stored in Zarr format. Beyond format, the sole difference between re-gridded data available therein and that through the Copernicus Climate Data Store (CDS) is variable naming: longnames are used in the former and shortnames in the latter.\n",
        "\n",
        "By default, subset data is written to the default blob storage container for the workspace, \"workspaceblobstore\".\n",
        "\n",
        "This notebook should be run in the Notebooks area of Azure Machine Learning. It does not require GPU capable compute and, due to lazy loading of extracted data and streamed write of the selected subset, is less memory intensive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# using the Python 3.10 - SDK v2 kernel\n",
        "%pip install xarray zarr fsspec gcsfs dask adlfs azure-ai-ml azure-identity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1767698924227
        }
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from datetime import datetime, timezone\n",
        "from pathlib import Path\n",
        "from uuid import uuid4\n",
        "\n",
        "import numpy as np\n",
        "import xarray as xr  # also requires zarr, fsspec, gcsfs, dask\n",
        "from adlfs import AzureBlobFileSystem\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.ai.ml.entities import Data\n",
        "from azure.core.exceptions import ResourceNotFoundError\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "# insert parent directory to path for proper absolute local imports\n",
        "sys.path.insert(0, str(Path.cwd().parent.parent.resolve()))\n",
        "from setup.common.utils import get_aml_ci_env_vars, get_latest_asset\n",
        "from setup.components.common.constants import (\n",
        "    ATMOS_LEVELS,\n",
        "    ATMOS_VAR_MAP,\n",
        "    STATIC_VAR_MAP,\n",
        "    SURF_VAR_MAP,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Define the GCP ERA5 dataset from which to extract a subset.\n",
        "\n",
        "**NOTE:** See the [GCP ERA5 ARCO bucket](https://console.cloud.google.com/storage/browser/gcp-public-data-arco-era5) for other datasets including alternatively gridded Zarr and raw source NetCDF files. Not all datasets contain every variable or the same time range / frequency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1767693139802
        }
      },
      "outputs": [],
      "source": [
        "GCP_ERA5_PATH = \"gs://gcp-public-data-arco-era5/ar/full_37-1h-0p25deg-chunk-1.zarr-v3\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Define parameters dictating timestamps to load, namely start and end date and timestep (frequency)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1767693139925
        }
      },
      "outputs": [],
      "source": [
        "START_DATE = datetime(2025, 1, 1, 0, tzinfo=timezone.utc).isoformat()\n",
        "END_DATE = datetime(2025, 1, 31, 23, tzinfo=timezone.utc).isoformat()\n",
        "FREQUENCY = 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Define the surface and pressure level variables and pressure levels to load. Variable longnames are mapped to shortnames for convenience (particularly when reading data into Aurora `Batch` objects) and are non-functional.\n",
        "\n",
        "To ingest new variables and levels, add the former by longname to the appropriate dictionary and the latter by integer pressure level (hPa) to the given list.\n",
        "\n",
        "**NOTE:** The two variable mappings are not strict. That is, single-level variables can be added to the pressure level variable mapping without error, they simply afford separation and readability. For the minimum set of variables required for Aurora 0.25 pre-trained, see `setup/components/common/constants.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1767693140090
        }
      },
      "outputs": [],
      "source": [
        "EXTRA_SFC_VARS = {\n",
        "    \"2m_dewpoint_temperature\": \"d2m\",\n",
        "}\n",
        "EXTRA_ATMOS_VARS = {}\n",
        "EXTRA_LEVELS = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Lazy load and subset data by variables, levels, time range, and timestep.\n",
        "\n",
        "**NOTE:** This will take at least 1 minute regardless of subset size due to the need to load metadata for this PB scale dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1767693201427
        }
      },
      "outputs": [],
      "source": [
        "ds = xr.open_zarr(GCP_ERA5_PATH, chunks={})\n",
        "variables = [\n",
        "    *SURF_VAR_MAP.keys(),\n",
        "    *STATIC_VAR_MAP.keys(),\n",
        "    *ATMOS_VAR_MAP.keys(),\n",
        "    *EXTRA_SFC_VARS.keys(),\n",
        "    *EXTRA_ATMOS_VARS.keys(),\n",
        "]\n",
        "var_subset_ds = ds[variables]\n",
        "subset_ds = var_subset_ds.sel(\n",
        "    time=slice(np.datetime64(START_DATE), np.datetime64(END_DATE), FREQUENCY),\n",
        "    level=ATMOS_LEVELS + EXTRA_LEVELS,\n",
        ")\n",
        "# update metadata attributes to reflect the subset, not original, data\n",
        "subset_ds.attrs.update(valid_time_start=START_DATE, valid_time_stop=END_DATE)\n",
        "subset_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Obtain and create necessary environment parameters and Azure interface objects.\n",
        "\n",
        "**NOTE:** when run on AML, the compute instance should have an identity with the Azure AI Administrator and Storage Blob Data Contributor roles. You may need to use the `azure.identity.ManagedIdentityCredential(client_id=\"...\")` class with a client ID referencing an identity assigned the aforementioned roles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1767698692521
        }
      },
      "outputs": [],
      "source": [
        "az_cred = DefaultAzureCredential()\n",
        "sub_id, rg_name, ws_name = get_aml_ci_env_vars()\n",
        "ml_client = MLClient(\n",
        "    credential=az_cred,\n",
        "    subscription_id=sub_id,\n",
        "    resource_group_name=rg_name,\n",
        "    workspace_name=ws_name,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Define location and write subset data using the default workspace blob storage container (\"workspaceblobstore\"), the corresponding storage account, and a UUID v4 store name to avoid inadvertent naming collisions.\n",
        "\n",
        "**NOTE:** The filesystem object and mapper can be avoided by using \"abfs://\" protocol paths and the `storage_options` parameter of `.to_zarr()`, though doing so can result in bugs from event loops created and managed by `xarray` / `zarr` and `fsspec` / `adlfs`. For example:\n",
        "```python\n",
        "subset_ds.to_zarr(\n",
        "    f\"abfs://{dst_datastore.container_name}/{uuid4()}.zarr\",\n",
        "    mode=\"w\",\n",
        "    compute=True,\n",
        "    consolidated=True,\n",
        "    zarr_format=2,\n",
        "    storage_options={\n",
        "        \"credential\": DefaultAzureCredential(),\n",
        "        \"account_name\": dst_datastore.account_name,\n",
        "    },\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1767698726646
        }
      },
      "outputs": [],
      "source": [
        "dst_datastore = ml_client.datastores.get(\"workspaceblobstore\")\n",
        "path = f\"aurora-workshop/input/{uuid4()}.zarr\"\n",
        "fs = AzureBlobFileSystem(dst_datastore.account_name, credential=az_cred)\n",
        "store = fs.get_mapper(f\"{dst_datastore.container_name}/{path}\")\n",
        "subset_ds.to_zarr(\n",
        "    store,\n",
        "    mode=\"w\",\n",
        "    compute=True,\n",
        "    consolidated=True,\n",
        "    zarr_format=2,\n",
        ")\n",
        "print(\n",
        "    f\"Wrote to: account={dst_datastore.account_name}, \"\n",
        "    f\"container={dst_datastore.container_name}, store={path}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Confirm persisted data is available and valid.\n",
        "\n",
        "**NOTE:** An equality check with the original `subset_ds` (e.g. `new_ds.equals(subset_ds)`) can be used for the avoidance of doubt but requires loading data into memory, which may take time and result in an OOM error, subset size dependent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1767698445416
        }
      },
      "outputs": [],
      "source": [
        "xr.open_dataset(store, engine=\"zarr\", chunks={})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Define and create / update the Azure Machine Learning data asset entity for persisted data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1767699222405
        }
      },
      "outputs": [],
      "source": [
        "asset_name = \"gcp-era5-arco\"\n",
        "try:\n",
        "    new = int(get_latest_asset(ml_client.data, asset_name).version) + 1\n",
        "except ResourceNotFoundError:\n",
        "    new = 1\n",
        "\n",
        "data_asset = Data(\n",
        "    name=asset_name,\n",
        "    version=str(new),\n",
        "    description=\"Zarr subset of ERA5 data from the GCP ERA5 ARCO dataset.\",\n",
        "    path=f\"azureml://subscriptions/{sub_id}/resourcegroups/{rg_name}/workspaces/{ws_name}/datastores/{dst_datastore.name}/paths/{path}\",\n",
        ")\n",
        "ml_client.data.create_or_update(data_asset)\n",
        "print(\n",
        "    f\"Created or updated asset: name={data_asset.name}, version={data_asset.version}, \"\n",
        "    f\"path={data_asset.path}\",\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "testbed-env"
    },
    "kernelspec": {
      "display_name": "aurora-introductory-workshop",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
